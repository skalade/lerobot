{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf762a4-0fb8-41bc-829d-28b9d97ba76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/miniconda3/envs/robot_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 10 files: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 127100.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "Success!\n",
      "Video of the evaluation is available in 'outputs/eval/example_act/rollout_int8.mp4'.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "\n",
    "# Create a directory to store the video of the evaluation\n",
    "output_directory = Path(\"outputs/eval/example_act\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the act policy for aloha environment\n",
    "pretrained_policy_path = Path(snapshot_download(\"lerobot/act_aloha_sim_transfer_cube_human\"))\n",
    "\n",
    "policy = ACTPolicy.from_pretrained(pretrained_policy_path)\n",
    "\n",
    "# Quantization\n",
    "policy_int8 = torch.ao.quantization.quantize_dynamic(\n",
    "    policy,  # the original model\n",
    "    {torch.nn.Linear, torch.nn.Conv2d},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)\n",
    "\n",
    "policy_int8.eval()\n",
    "\n",
    "# Only cpu for experiments\n",
    "device = torch.device(\"cpu\")\n",
    "policy_int8.to(device);\n",
    "\n",
    "import os\n",
    "os.environ[\"MUJOCO_GL\"]=\"egl\"\n",
    "import gym_aloha\n",
    "\n",
    "env = gym.make(\n",
    "    \"gym_aloha/AlohaTransferCube-v0\",\n",
    "    obs_type=\"pixels_agent_pos\"\n",
    ")\n",
    "\n",
    "# Reset the policy and environmens to prepare for rollout\n",
    "policy_int8.reset()\n",
    "numpy_observation, info = env.reset(seed=43)\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    # Prepare observation for the policy running in Pytorch\n",
    "    state = torch.from_numpy(numpy_observation['agent_pos'])\n",
    "    state = state.to(torch.float32)\n",
    "    image = torch.from_numpy(numpy_observation['pixels']['top'])\n",
    "    image = image.to(torch.float32) / 255\n",
    "    image = image.permute(2, 0, 1)\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to(device, non_blocking=True)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = state.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation = {\n",
    "        \"observation.images.top\": image,\n",
    "        \"observation.state\": state,\n",
    "    }\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action = policy_int8.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    # print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"rollout_int8.mp4\"\n",
    "imageio.mimsave(str(video_path), numpy.stack(frames), fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc6387-95f0-494b-9a55-324785b81cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
