{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf762a4-0fb8-41bc-829d-28b9d97ba76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/miniconda3/envs/robot_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 10 files: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 157680.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "\n",
    "# Create a directory to store the video of the evaluation\n",
    "output_directory = Path(\"outputs/eval/example_act\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the act policy for aloha environment\n",
    "pretrained_policy_path = Path(snapshot_download(\"lerobot/act_aloha_sim_transfer_cube_human\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c58d4e-e475-4ab8-a963-12dc46dabf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ACTPolicy(\n",
       "  (normalize_inputs): Normalize(\n",
       "    (buffer_observation_images_top): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.FloatTensor of size 3x1x1]\n",
       "        (std): Parameter containing: [torch.FloatTensor of size 3x1x1]\n",
       "    )\n",
       "    (buffer_observation_state): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.FloatTensor of size 14]\n",
       "        (std): Parameter containing: [torch.FloatTensor of size 14]\n",
       "    )\n",
       "  )\n",
       "  (normalize_targets): Normalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.FloatTensor of size 14]\n",
       "        (std): Parameter containing: [torch.FloatTensor of size 14]\n",
       "    )\n",
       "  )\n",
       "  (unnormalize_outputs): Unnormalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.FloatTensor of size 14]\n",
       "        (std): Parameter containing: [torch.FloatTensor of size 14]\n",
       "    )\n",
       "  )\n",
       "  (model): ACT(\n",
       "    (vae_encoder): ACTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ACTEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (vae_encoder_cls_embed): Embedding(1, 512)\n",
       "    (vae_encoder_robot_state_input_proj): Linear(in_features=14, out_features=512, bias=True)\n",
       "    (vae_encoder_action_input_proj): Linear(in_features=14, out_features=512, bias=True)\n",
       "    (vae_encoder_latent_output_proj): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (backbone): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): ACTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ACTEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (decoder): ACTDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ACTDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=3200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=3200, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder_robot_state_input_proj): Linear(in_features=14, out_features=512, bias=True)\n",
       "    (encoder_latent_input_proj): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (encoder_img_feat_input_proj): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (encoder_1d_feature_pos_embed): Embedding(2, 512)\n",
       "    (encoder_cam_feat_pos_embed): ACTSinusoidalPositionEmbedding2d()\n",
       "    (decoder_pos_embed): Embedding(100, 512)\n",
       "    (action_head): Linear(in_features=512, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = ACTPolicy.from_pretrained(pretrained_policy_path)\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4650f154-5273-4d71-840e-aabc75048e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Device set to:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU is not available. Device set to: {device}. Inference will be slower than on GPU.\")\n",
    "    # Decrease the number of reverse-diffusion steps (trades off a bit of quality for 10x speed)\n",
    "    policy.diffusion.num_inference_steps = 10\n",
    "\n",
    "policy.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7b61d6-4c2a-4c85-9b49-40b599ac27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"]=\"egl\"\n",
    "import gym_aloha\n",
    "\n",
    "env = gym.make(\n",
    "    \"gym_aloha/AlohaTransferCube-v0\",\n",
    "    obs_type=\"pixels_agent_pos\"\n",
    ")\n",
    "\n",
    "# Reset the policy and environmens to prepare for rollout\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c69fdf-ad68-446c-a0fb-25551ba22e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "Video of the evaluation is available in 'outputs/eval/example_act/rollout.mp4'.\n"
     ]
    }
   ],
   "source": [
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    # Prepare observation for the policy running in Pytorch\n",
    "    state = torch.from_numpy(numpy_observation['agent_pos'])\n",
    "    state = state.to(torch.float32)\n",
    "    image = torch.from_numpy(numpy_observation['pixels']['top'])\n",
    "    image = image.to(torch.float32) / 255\n",
    "    image = image.permute(2, 0, 1)\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to(device, non_blocking=True)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = state.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation = {\n",
    "        \"observation.images.top\": image,\n",
    "        \"observation.state\": state,\n",
    "    }\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    # print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), numpy.stack(frames), fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
